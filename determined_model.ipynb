{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import time\n",
    "import datetime\n",
    "# from itertools import combinations, permutations\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lmap(func, iterable):\n",
    "    '''\n",
    "    lmap(func, iterable) -> list of funcs applied to elements\n",
    "    \n",
    "    same as [ func(item) for item in iterable ]\n",
    "    '''\n",
    "    return list(map(func, iterable))\n",
    "\n",
    "\n",
    "\n",
    "def get_min_max(seq):\n",
    "    '''\n",
    "    simple min max scaler\n",
    "    '''\n",
    "    \n",
    "    min_ = min(seq)\n",
    "    max_ = max(seq)\n",
    "    \n",
    "    return (seq-min_)/(max_ - min_)\n",
    "    \n",
    "\n",
    "#\n",
    "#\n",
    "# make df\n",
    "#\n",
    "#\n",
    "    \n",
    "cols = [\"cores\", \"memory\", \"price_per_cpu_hr\", 'processor_clockspeed', 'cache_size']\n",
    "\n",
    "# define the coeficients\n",
    "magic_nums = OrderedDict()\n",
    "for col in cols:\n",
    "    magic_nums[col] = random.randint(-10,10)*random.random()\n",
    "\n",
    "\n",
    "TUPLEN = len(cols)\n",
    "NROWS=100000\n",
    "MAXVAL=128 \n",
    "\n",
    "\n",
    "\n",
    "def get_random_Ntuples(N=TUPLEN,maxval=MAXVAL, return_floats=False):\n",
    "    \n",
    "    rands = []\n",
    "    \n",
    "\n",
    "    for _ in range(N):\n",
    "        rands.append(random.randint(1, maxval))\n",
    "        \n",
    "    if return_floats:\n",
    "        rands = lmap(float, rands)\n",
    "        \n",
    "    \n",
    "    return tuple(rands)\n",
    "\n",
    "\n",
    "\n",
    "data = [ get_random_Ntuples() for _ in range(NROWS) ]\n",
    "df = pd.DataFrame(columns=cols, data=data)\n",
    "\n",
    "\n",
    "outcomes = []\n",
    "\n",
    "for idx,  row in df.iterrows():\n",
    "    \n",
    "    this_outcome = 0\n",
    "    \n",
    "    for idxx, col in enumerate(cols):\n",
    "        \n",
    "        this_outcome += magic_nums[col]*row[col]\n",
    "    \n",
    "    outcomes.append(this_outcome)\n",
    "        \n",
    "    \n",
    "\n",
    "df['outcomes'] = outcomes\n",
    "\n",
    "# df_scaled = df.apply(get_min_max)\n",
    "\n",
    "#print(df.iloc[0:10])\n",
    "# print(df_scaled.iloc[0:10])\n",
    "# print()\n",
    "\n",
    "# df_scaled\n",
    "\n",
    "# partition into 3/4 train, 1/4 predict\n",
    "# after scrambing order\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "df_scaled = df.apply(get_min_max)\n",
    "\n",
    "\n",
    "LENDF      = len(df)\n",
    "inflection = LENDF*3//4\n",
    "inflection2 = len(df)\n",
    "training_df = df.iloc[0:inflection]\n",
    "predict_df  = df.iloc[inflection:]\n",
    "\n",
    "# print(len(training_set), len(predict_on))\n",
    "\n",
    "\n",
    "# def make_df_into_feature_arrays(input_df, feature_cols=None):\n",
    "#     if feature_cols is None:    \n",
    "#         return np.array([ np.array(list((x))) for x in input_df.to_numpy() ])\n",
    "#     else:\n",
    "#         return np.array([ np.array(list((x))) for x in input_df[feature_cols].to_numpy() ])\n",
    "    \n",
    "\n",
    "# traindf = make_df_into_feature_arrays()\n",
    "    \n",
    "\n",
    "    \n",
    "# Models       = [ LinearRegression(),   LassoCV(),   RidgeCV(),   LinearSVR()]\n",
    "# model_names = ['LinearRegression()', 'LassoCV()', 'RidgeCV()', 'LinearSVR()']\n",
    "\n",
    "\n",
    "\n",
    "# delim='#'*80\n",
    "\n",
    "# def make_delim():\n",
    "#     print(delim)\n",
    "    \n",
    "# for idx, _ in enumerate(Models):\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model = _\n",
    "    \n",
    "#     model.fit(x_train, y_train)\n",
    "    \n",
    "#     model.predict(x_pred, y_pred)\n",
    "    \n",
    "#     model_name = model_names[idx]\n",
    "    \n",
    "#     score = model.accuracy_score\n",
    "    \n",
    "#     print(make_delim)\n",
    "#     print(f'model is {madel_name} and then accuracy scorec is {score}.')\n",
    "#     print(make_delim)\n",
    "    \n",
    "#     assert len(x_pred) == len(y_pred)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# # for col in df.columns:\n",
    "# #     df[col] = lmap(minmax_scale,list(df[col]))\n",
    "\n",
    "# # df['outcome'] = [ float(random.randint(0,10))/10 for _ in range(len(df)) ]\n",
    "\n",
    "# # print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy classification score.\n",
      "\n",
      "    In multilabel classification, this function computes subset accuracy:\n",
      "    the set of labels predicted for a sample must *exactly* match the\n",
      "    corresponding set of labels in y_true.\n",
      "\n",
      "    Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) labels.\n",
      "\n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Predicted labels, as returned by a classifier.\n",
      "\n",
      "    normalize : bool, optional (default=True)\n",
      "        If ``False``, return the number of correctly classified samples.\n",
      "        Otherwise, return the fraction of correctly classified samples.\n",
      "\n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    score : float\n",
      "        If ``normalize == True``, return the fraction of correctly\n",
      "        classified samples (float), else returns the number of correctly\n",
      "        classified samples (int).\n",
      "\n",
      "        The best performance is 1 with ``normalize == True`` and the number\n",
      "        of samples with ``normalize == False``.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    jaccard_score, hamming_loss, zero_one_loss\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    In binary and multiclass classification, this function is equal\n",
      "    to the ``jaccard_score`` function.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import accuracy_score\n",
      "    >>> y_pred = [0, 2, 1, 3]\n",
      "    >>> y_true = [0, 1, 2, 3]\n",
      "    >>> accuracy_score(y_true, y_pred)\n",
      "    0.5\n",
      "    >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "    2\n",
      "\n",
      "    In the multilabel case with binary label indicators:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "    0.5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000 25000\n",
      "(75000, 5)\n",
      "(75000,)\n",
      "(25000, 5)\n",
      "(25000,)\n",
      "################################################################################\n",
      "Model if LinearRegression():\n",
      "################################################################################\n",
      "name: LinearRegression()\n",
      "coefficients: [-3.35859170e-16 -8.59994991e-01  1.95339761e+00 -7.12765595e-01\n",
      " -4.13891774e-01]\n",
      "magic numbers: [0.0, -0.859994990999227, 1.9533976130337511, -0.7127655945940594, -0.41389177375674435]\n",
      "score: None\n",
      "quick: 1.0, frelative: (9.80752923318562e-10, -1.6639929212827884e-14)\n",
      "\n",
      "deltas: [-3.35859170e-16 -7.77156117e-16 -4.44089210e-16  2.22044605e-16\n",
      "  4.99600361e-16]\n",
      "################################################################################\n",
      "\n",
      "################################################################################\n",
      "Model if LassoCV():\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/altoidnerd/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "name: LassoCV()\n",
      "coefficients: [ 0.         -0.85804168  1.95144688 -0.710828   -0.41195968]\n",
      "magic numbers: [0.0, -0.859994990999227, 1.9533976130337511, -0.7127655945940594, -0.41389177375674435]\n",
      "score: None\n",
      "quick: 0.94756, frelative: (2918.3599749952614, -0.0495143091056563)\n",
      "\n",
      "deltas: [ 0.          0.00195331 -0.00195073  0.00193759  0.0019321 ]\n",
      "################################################################################\n",
      "\n",
      "################################################################################\n",
      "Model if RidgeCV():\n",
      "################################################################################\n",
      "name: RidgeCV()\n",
      "coefficients: [-4.28310220e-07 -8.59993607e-01  1.95339510e+00 -7.12764725e-01\n",
      " -4.13891165e-01]\n",
      "magic numbers: [0.0, -0.859994990999227, 1.9533976130337511, -0.7127655945940594, -0.41389177375674435]\n",
      "score: None\n",
      "quick: 1.0, frelative: (2.354142445309907, -3.994152116755562e-05)\n",
      "\n",
      "deltas: [-4.28310220e-07  1.38429592e-06 -2.51022220e-06  8.69693531e-07\n",
      "  6.08478932e-07]\n",
      "################################################################################\n",
      "\n",
      "################################################################################\n",
      "Model if LinearSVR():\n",
      "################################################################################\n",
      "name: LinearSVR()\n",
      "coefficients: [ 9.23209549e-09 -8.59994975e-01  1.95339763e+00 -7.12765589e-01\n",
      " -4.13891758e-01]\n",
      "magic numbers: [0.0, -0.859994990999227, 1.9533976130337511, -0.7127655945940594, -0.41389177375674435]\n",
      "score: None\n",
      "quick: 1.0, frelative: (0.025603766699759913, -4.3440590931318553e-07)\n",
      "\n",
      "deltas: [9.23209549e-09 1.58981246e-08 1.65670235e-08 5.99539485e-09\n",
      " 1.58981590e-08]\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(training_df), len(predict_df))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_df_into_feature_arrays(input_df, feature_cols=None):\n",
    "    if feature_cols is None:    \n",
    "        return np.array([ np.array(list((x))) for x in input_df.to_numpy() ])\n",
    "    else:\n",
    "        return np.array([ np.array(list((x))) for x in input_df[feature_cols].to_numpy() ])\n",
    "    \n",
    "dependents  = ['cores', 'memory', 'price_per_cpu_hr', 'processor_clockspeed','cache_size']\n",
    "independent = ['outcomes']\n",
    "\n",
    "X_TRAIN = make_df_into_feature_arrays(training_df[dependents])\n",
    "Y_TRAIN = training_df[independent].outcomes.values\n",
    "\n",
    "\n",
    "\n",
    "X_PREDICT = make_df_into_feature_arrays(predict_df[dependents])\n",
    "Y_ACTUAL  = predict_df[independent].outcomes.values\n",
    "\n",
    "\n",
    "for item in [X_TRAIN, Y_TRAIN, X_PREDICT, Y_ACTUAL]:\n",
    "    print(item.shape)\n",
    "\n",
    "    \n",
    "Models       = [ LinearRegression(),   LassoCV(),   RidgeCV(),   LinearSVR()]\n",
    "Model_names = ['LinearRegression()', 'LassoCV()', 'RidgeCV()', 'LinearSVR()']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_delim(size_in_chars=80):\n",
    "    delim='#'*size_in_chars\n",
    "    print(delim)\n",
    "    \n",
    "def quick_score(y1, y2, ATOL=1e-3, RTOL=1e-2):\n",
    "\n",
    "    assert len(y1) == len(y2)\n",
    "\n",
    "    results = np.isclose(y1,y2, atol=ATOL, rtol=RTOL)\n",
    "    correct = len([ i for i in results if i ])\n",
    "    incorrect = len(y1) - correct\n",
    "\n",
    "    return correct/len(results)\n",
    "\n",
    "\n",
    "\n",
    "def diff_score(y1,y2):\n",
    "    '''\n",
    "    def homebrew_score2(y1,y2):\n",
    "    returns (absolute_error, relative_error) as tuple\n",
    "    '''\n",
    "    \n",
    "    diffs = [] \n",
    "    \n",
    "    for idx, item in enumerate(y1):\n",
    "        \n",
    "        delta = abs(item-y2[idx])\n",
    "        \n",
    "        diffs.append(delta)\n",
    "        \n",
    "    absolute_error = sum(diffs)\n",
    "    relative_error = absolute_error/sum(y2)\n",
    "    \n",
    "    return (absolute_error, relative_error)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for idx, _ in enumerate(Models):\n",
    "    \n",
    "    model = _\n",
    "    \n",
    "    show_delim()\n",
    "    name = Model_names[idx]\n",
    "    print(f'Model if {name}:')\n",
    "    \n",
    "    model.fit(X_TRAIN,Y_TRAIN)\n",
    "    \n",
    "    Y_PREDICTED = model.predict(X_PREDICT)\n",
    "    \n",
    "    model_name = Model_names[idx]\n",
    "    \n",
    "    model_coefs = model.coef_\n",
    "    \n",
    "    score = 'None'\n",
    "    \n",
    "#     score = model.score(Y_ACTUAL, Y_PREDICTED)\n",
    "    \n",
    "    quick = quick_score(Y_PREDICTED,Y_ACTUAL)\n",
    "    \n",
    "    rel   = diff_score(Y_PREDICTED, Y_ACTUAL)\n",
    "    \n",
    "    magic = [ magic_nums[col] for col in cols]\n",
    "    \n",
    "    deltas = np.array(model_coefs) - np.array(magic)\n",
    "    \n",
    "    show_delim()\n",
    "    \n",
    "    print(f\"name: {model_name}\", f'coefficients: {model_coefs}', f'magic numbers: {magic}', f'score: {score}', f'quick: {quick}, frelative: {rel}', sep='\\n')\n",
    "    print()\n",
    "    print(f'deltas: {deltas}')\n",
    "\n",
    "    \n",
    "    #print(f'model is {madel_name} and then accuracy scorec is {score}.')\n",
    "    show_delim()\n",
    "    print()\n",
    "    \n",
    "    assert len(Y_ACTUAL) == len(Y_PREDICTED)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# # for col in df.columns:\n",
    "# #     df[col] = lmap(minmax_scale,list(df[col]))\n",
    "\n",
    "# # df['outcome'] = [ float(random.randint(0,10))/10 for _ in range(len(df)) ]\n",
    "\n",
    "# # print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_PREDICTED.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-17.21730737  73.34709855 -43.05508195 ... -34.38649769 -74.94150999\n -48.31941108].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-5823f0c4bda4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ACTUAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PREDICTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m    206\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-17.21730737  73.34709855 -43.05508195 ... -34.38649769 -74.94150999\n -48.31941108].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "model.score(Y_ACTUAL, Y_PREDICTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy classification score.\\n\\n    In multilabel classification, this function computes subset accuracy:\\n    the set of labels predicted for a sample must *exactly* match the\\n    corresponding set of labels in y_true.\\n\\n    Read more in the :ref:`User Guide <accuracy_score>`.\\n\\n    Parameters\\n    ----------\\n    y_true : 1d array-like, or label indicator array / sparse matrix\\n        Ground truth (correct) labels.\\n\\n    y_pred : 1d array-like, or label indicator array / sparse matrix\\n        Predicted labels, as returned by a classifier.\\n\\n    normalize : bool, optional (default=True)\\n        If ``False``, return the number of correctly classified samples.\\n        Otherwise, return the fraction of correctly classified samples.\\n\\n    sample_weight : array-like of shape = [n_samples], optional\\n        Sample weights.\\n\\n    Returns\\n    -------\\n    score : float\\n        If ``normalize == True``, return the fraction of correctly\\n        classified samples (float), else returns the number of correctly\\n        classified samples (int).\\n\\n        The best performance is 1 with ``normalize == True`` and the number\\n        of samples with ``normalize == False``.\\n\\n    See also\\n    --------\\n    jaccard_score, hamming_loss, zero_one_loss\\n\\n    Notes\\n    -----\\n    In binary and multiclass classification, this function is equal\\n    to the ``jaccard_score`` function.\\n\\n    Examples\\n    --------\\n    >>> from sklearn.metrics import accuracy_score\\n    >>> y_pred = [0, 2, 1, 3]\\n    >>> y_true = [0, 1, 2, 3]\\n    >>> accuracy_score(y_true, y_pred)\\n    0.5\\n    >>> accuracy_score(y_true, y_pred, normalize=False)\\n    2\\n\\n    In the multilabel case with binary label indicators:\\n\\n    >>> import numpy as np\\n    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n    0.5\\n    '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-17.21730737],\n",
       "       [ 73.34709855],\n",
       "       [-43.05508195],\n",
       "       ...,\n",
       "       [-34.38649769],\n",
       "       [-74.94150999],\n",
       "       [-48.31941108]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_PREDICTED.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1308.5411692  1438.87830549  662.02015943 ...  439.71993331  594.53085846\n  939.8894265 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-cfbeaeb001fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ACTUAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PREDICTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy_acore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_acore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ACTUAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_PREDICTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m    206\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1308.5411692  1438.87830549  662.02015943 ...  439.71993331  594.53085846\n  939.8894265 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "score = model.score(Y_ACTUAL, Y_PREDICTED)\n",
    "    \n",
    "accuracy_acore = model.accuracy_acore(Y_ACTUAL, Y_PREDICTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-48a6c01e0773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: score() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l1=[1,3,5,6,6.07]\n",
    "l2=[1,3,5,6,6]\n",
    "\n",
    "diff_score(l2,l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose([1,2,3],[1,2.00001,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
